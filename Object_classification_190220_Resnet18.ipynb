{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import copy\n",
    "import time\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_box_in_box(test_list, log_enable=False):\n",
    "    final_list = []\n",
    "    bypass_flag_1 = 0\n",
    "    for i in range(len(test_list)):\n",
    "        bypass_flag = 0\n",
    "        if bypass_flag_1:\n",
    "            bypass_flag_1 = 0\n",
    "            continue\n",
    "        if log_enable:\n",
    "            print('i:',i, test_list[i])\n",
    "            print('*'*35)\n",
    "        for j in range(len(test_list)-i-1):\n",
    "            if log_enable:\n",
    "                print('[j+i+1]:',j+i+1, test_list[j+i+1])\n",
    "            # check box in box condition in 2 cases\n",
    "            if (test_list[i][0] < test_list[j+i+1][0] and \n",
    "                test_list[i][1] < test_list[j+i+1][1] and \n",
    "                test_list[i][0] + test_list[i][2] >= test_list[j+i+1][0] + test_list[j+i+1][2] and \n",
    "                test_list[i][1] + test_list[i][3] >= test_list[j+i+1][1] + test_list[j+i+1][3]):\n",
    "                if log_enable:\n",
    "                    print(\"Found this box {} in current box {}!!\".format(test_list[j+i+1], test_list[i]))\n",
    "                    print(\"Drop this box {}\".format(test_list[j+i+1]))\n",
    "                bypass_flag_1 = 1\n",
    "            elif (test_list[i][0] > test_list[j+i+1][0] and \n",
    "                  test_list[i][1] > test_list[j+i+1][1] and \n",
    "                  test_list[i][0] + test_list[i][2] <= test_list[j+i+1][0] + test_list[j+i+1][2] and \n",
    "                  test_list[i][1] + test_list[i][3] <= test_list[j+i+1][1] + test_list[j+i+1][3]):\n",
    "                if log_enable:\n",
    "                    print(\"Found current box {} in this box {}!!\".format(test_list[i], test_list[j+i+1]))\n",
    "                    print(\"Drop current box {}\".format(test_list[i]))\n",
    "                bypass_flag = 1\n",
    "                break\n",
    "        if log_enable:\n",
    "            print('*'*35)\n",
    "        if bypass_flag:\n",
    "            continue\n",
    "        final_list.append(test_list[i])\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_filter(boxs):\n",
    "    boxs_f = []\n",
    "    if len(boxs) > 0:\n",
    "        for box in boxs:\n",
    "            if box[2] <80 and box[3]<80: #filter small size(<80) boxs\n",
    "                continue\n",
    "            elif box[3] > 350 or box[2]*box[3] > 50000:\n",
    "                continue\n",
    "            elif box[0] + box[2] > 620:\n",
    "                continue\n",
    "            elif box[0] < 20:\n",
    "                continue\n",
    "            elif box[1] + box[3] > 460:   \n",
    "                continue\n",
    "            if box[1] < 5 or box[0] > 600:\n",
    "                continue\n",
    "            else:\n",
    "                boxs_f.append(box)\n",
    "        #print('boxs_f:', boxs_f)\n",
    "        return boxs_f\n",
    "    else:\n",
    "        print('no boxs found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def six_box_color(box_cnt):\n",
    "    if box_cnt % 6 == 0:\n",
    "        color = (255, 0, 255)\n",
    "    elif box_cnt % 6 ==1:\n",
    "        color = (255, 255, 0)\n",
    "    elif box_cnt % 6 ==2:\n",
    "        color = (0, 255, 255)\n",
    "    elif box_cnt % 6 ==3:\n",
    "        color = (0, 255, 0)\n",
    "    elif box_cnt % 6 ==4:\n",
    "        color = (0, 0, 255)\n",
    "    else:\n",
    "        color = (255, 0, 0)\n",
    "    return color\n",
    "\n",
    "def plot_boxs(frame0, boxs):\n",
    "    #new_frame = frame0.copy()\n",
    "    box_cnt = 0\n",
    "    if len(boxs):\n",
    "        for box in boxs:\n",
    "            color = six_box_color(box_cnt)\n",
    "            text = 'object'+str(box_cnt+1)\n",
    "            box_cnt+=1\n",
    "            cv2.rectangle(frame0,(box[0],box[1]),(box[0]+box[2],box[1]+box[3]),color,2)\n",
    "            cv2.rectangle(frame0,(box[0]-1,box[1]-18),(box[0]+box[2]//3*2,box[1]),color,-1)\n",
    "            cv2.putText(frame0, text, (box[0], box[1]-5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.4, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "        #return frame0\n",
    "    else:\n",
    "        print('no boxs found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#crop_image >> process image with particular bounding box\n",
    "def crop_image_pre(img, box):\n",
    "    new_img = np.zeros_like(img)\n",
    "    w = box[2]\n",
    "    h = box[3]\n",
    "    x = (img.shape[1] - w) //2\n",
    "    y = (img.shape[0] - h) //2\n",
    "    new_img[y:y+h,x:x+w,:] = img[box[1]:box[1]+box[3],box[0]:box[0]+box[2],:]\n",
    "    return new_img\n",
    "# wrap crop image with box list\n",
    "def crop_img_with_box_list(img, box_list, model,\n",
    "                           save_crop_image=False, \n",
    "                           show_img_on_jupyter=False,\n",
    "                           prediction=False):\n",
    "    result_img = img.copy()\n",
    "    cls_list = []\n",
    "    probs_list = []\n",
    "    for idx, box in enumerate(box_list):\n",
    "        img_crop = crop_image_pre(img, box)\n",
    "        if save_crop_image:\n",
    "            cv2.imwrite('./crop_'+str(idx+1)+'.jpg', img_crop)\n",
    "        if show_img_on_jupyter:\n",
    "            plt.figure(figsize=[7,7])\n",
    "            #plt.subplot(4,3,idx+1)\n",
    "            plt.imshow(cv2.cvtColor(img_crop, cv2.COLOR_BGR2RGB))\n",
    "            plt.xticks([]), plt.yticks([])\n",
    "        if prediction:\n",
    "            print('Do prediction now')\n",
    "            try: \n",
    "                probs, cls = model_predict(img_crop, model, topk=1)\n",
    "                color = six_box_color(idx)\n",
    "                cv2.rectangle(result_img,(box[0],box[1]),(box[0]+box[2],box[1]+box[3]),color,2)\n",
    "                cv2.rectangle(result_img,(box[0]-1,box[1]-18),(box[0]+box[2]//4*3,box[1]),color,-1)\n",
    "                cv2.putText(result_img, cls[0]+'_' + str(probs[0]), (box[0], box[1]-5), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                            0.4, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "                cls_list.append(cls[0])\n",
    "                probs_list.append(probs[0])\n",
    "                \n",
    "            except:\n",
    "                print('prediction error')\n",
    "    return (cls_list, probs_list), result_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_model(filename ='checkpoint.pth', device =torch.device(\"cpu\")):\n",
    "    \n",
    "    if device == torch.device(\"cpu\"):\n",
    "        checkpoint = torch.load(filename, map_location=device)\n",
    "    elif device == torch.device('cuda'):\n",
    "        #checkpoint = torch.load(filename, map_location=\"cuda:0\")\n",
    "        checkpoint = torch.load(filename)\n",
    "    else:\n",
    "        print('Error!! checkpoint read fail, check your device setting.')\n",
    "        return 1\n",
    "       \n",
    "    model = models.resnet18(pretrained=True) #recall pre-train mode vgg16\n",
    "    for params in model.parameters():     #freeze pre-train model parameters \n",
    "        params.require_grad = False\n",
    "    print('building resnet18 pretrain_model...')#message\n",
    "    model.to(device)                      #set computaion unit  \n",
    "    \n",
    "    model.fc = nn.Sequential(\n",
    "                        nn.Linear(512, 256),\n",
    "                        nn.ReLU(),\n",
    "                        nn.Dropout(),\n",
    "                        nn.Linear(256, 14),\n",
    "                        nn.LogSoftmax(dim=1)    \n",
    "                    )\n",
    "    print('building model classifier with full connection:', #message\n",
    "          '\\ninput_size:512',\n",
    "          '\\nhidden_size:256',\n",
    "          '\\noutput_size:14'\n",
    "         )\n",
    "    dummy ={} #dummy diction \n",
    "    if type(checkpoint) == type(dummy):   #comfirm checkpoint is type of diction\n",
    "        if 'state_dict' in list(checkpoint.keys()):\n",
    "            print('update model stat_dict...')\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "        else:\n",
    "            print('no state_dict in this checkpoint')\n",
    "        if 'class_to_idx' in list(checkpoint.keys()):\n",
    "            print('update model class_to_idx...')\n",
    "            model.class_to_idx = checkpoint['class_to_idx']\n",
    "        else:\n",
    "            print('no class_to_idx in this checkpoint')\n",
    "    else:\n",
    "        print('no stat_dict & class_to_idx could be updated',\n",
    "              'you should retrain this model again.'\n",
    "             )\n",
    "    print('rebuild model finished!!')  #message\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv2 resize to keep aspect ratio\n",
    "def image_resize(image, width = None, height = None, inter = cv2.INTER_AREA):\n",
    "    # initialize the dimensions of the image to be resized and\n",
    "    # grab the image size\n",
    "    dim = None\n",
    "    (h, w) = image.shape[:2]\n",
    "\n",
    "    # if both the width and height are None, then return the\n",
    "    # original image\n",
    "    if width is None and height is None:\n",
    "        return image\n",
    "\n",
    "    # check to see if the width is None\n",
    "    if width is None:\n",
    "        # calculate the ratio of the height and construct the\n",
    "        # dimensions\n",
    "        r = height / float(h)\n",
    "        dim = (int(w * r), height)\n",
    "\n",
    "    # otherwise, the height is None\n",
    "    else:\n",
    "        # calculate the ratio of the width and construct the\n",
    "        # dimensions\n",
    "        r = width / float(w)\n",
    "        dim = (width, int(h * r))\n",
    "\n",
    "    # resize the image\n",
    "    resized = cv2.resize(image, dim, interpolation = inter)\n",
    "\n",
    "    # return the resized image\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_preprocess_for_model(np_img_or_filepath, resize=256, \n",
    "                          center_crop_size=224, plot_img=False):\n",
    "    '''image preprocess for pytorch model'''\n",
    "    # load img from np.array or from file\n",
    "    try:\n",
    "        img_cv = cv2.imread(np_img_or_filepath)\n",
    "    \n",
    "    except TypeError:\n",
    "        img_cv = np_img_or_filepath\n",
    "\n",
    "    #resize shorter edge to 256 \n",
    "    if img_cv.shape[0] <= img_cv.shape[1]:\n",
    "        img_resize = image_resize(img_cv, height=resize, inter=cv2.INTER_AREA)\n",
    "    else:\n",
    "        img_resize = image_resize(img_cv, width=resize, inter=cv2.INTER_AREA)\n",
    "        \n",
    "    # plot original size img and crop img size    \n",
    "    if plot_img:\n",
    "        plt.figure(figsize=[20,30])\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(cv2.cvtColor(img_cv, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"original size:{}\".format(img_cv.shape[:2]))\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(cv2.cvtColor(img_resize, cv2.COLOR_BGR2RGB))\n",
    "        plt.title(\"crop size:{}\".format(img_resize.shape[:2]))\n",
    "\n",
    "    #center crop to img_size (224, 224)\n",
    "    y_min = (img_resize.shape[0] -center_crop_size) //2\n",
    "    x_min = (img_resize.shape[1] -center_crop_size) //2\n",
    "    y_max = (img_resize.shape[0] -center_crop_size) //2 + center_crop_size\n",
    "    x_max = (img_resize.shape[1] -center_crop_size) //2 + center_crop_size\n",
    "    img_center_crop = np.zeros((center_crop_size, center_crop_size, 3), dtype='uint8')\n",
    "    img_center_crop = img_resize[y_min:y_max, x_min:x_max,:]\n",
    "    \n",
    "    # convet color chanel from BGR to RGB\n",
    "    img_RGB = cv2.cvtColor(img_center_crop, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Nomalization with means and stderr\n",
    "    means = np.array([0.485, 0.456, 0.406])\n",
    "    stderr = np.array([0.229, 0.224, 0.225])\n",
    "    nor_img = (img_RGB/255 - means) / stderr\n",
    "    \n",
    "    if plot_img:\n",
    "        plt.figure(figsize=[20,30])\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(img_RGB)\n",
    "        plt.title(\"Befor Nomarlization size:{}\".format(img_RGB.shape[:2]))\n",
    "        plt.subplot(122)\n",
    "        # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\n",
    "        #image = np.clip(image, 0, 1)\n",
    "        plt.imshow(np.clip(nor_img, 0, 1))\n",
    "        plt.title(\"After Nomarlization size:{}\".format(nor_img.shape[:2]))\n",
    "    \n",
    "    # transpose color chanel to first dimension for pytorch tensor\n",
    "    img_trans = nor_img.transpose((2, 1, 0))\n",
    "    \n",
    "    return img_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bounding_box_pre(img, save_path='',show_binary=False, binary_thred = 90,\n",
    "                 can1=10, can2=150, can3=1, \n",
    "                 dilate0=0, erode1=0, \n",
    "                 dilate1=0, erode2=0, \n",
    "                 dilate2=0, choose=0):\n",
    "    #img = cv2.imread(imgpath)\n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    #_, binary = cv2.threshold(gray_img, binary_thred, 255,cv2.THRESH_BINARY)\n",
    "    binary = cv2.adaptiveThreshold(gray_img, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY_INV, 11,25)\n",
    "    #_, binary = cv2.threshold(gray_img, binary_thred, 255,cv2.THRESH_BINARY)\n",
    "    edges = cv2.Canny(img, can1, can2, can3)\n",
    "    bin_edge = cv2.add(binary, edges)\n",
    "    edge_0 = cv2.dilate(bin_edge, np.ones((3,3)), iterations=dilate0)\n",
    "    edge_1 = cv2.erode(edge_0, np.ones((3,3)), iterations=erode1)\n",
    "    edge_2 = cv2.dilate(edge_1, np.ones((3,3)), iterations=dilate1)\n",
    "    edge_3 = cv2.erode(edge_2, np.ones((3,3)), iterations=erode2)\n",
    "    edge_4 = cv2.dilate(edge_3, np.ones((3,3)), iterations=dilate2)\n",
    "    \n",
    "    #cv2.imshow('total', (edges, edge_0, edge_1, edge_2))\n",
    "\n",
    "    if show_binary:\n",
    "        cv2.imshow(\"Binary\", binary)\n",
    "        cv2.imshow(\"Edge\", edges)\n",
    "        cv2.imshow(\"Bin+Edge:\", bin_edge)\n",
    "        cv2.imshow('dilate0', edge_0)\n",
    "        #cv2.imshow('errode1', edge_1)\n",
    "        #cv2.imshow('dilate1', edge_2)\n",
    "        #cv2.imshow('errode2', edge_3)\n",
    "        #cv2.imshow('dilate2', edge_4)\n",
    "    \n",
    "    #img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    _, cnts, _=cv2.findContours(edge_4, 1,2)\n",
    "    box_list = []\n",
    "    if len(cnts) > 0:\n",
    "        for cnt in cnts:        \n",
    "            x,y,w,h = cv2.boundingRect(cnt)\n",
    "            box_list.append((x,y,w,h))\n",
    "\n",
    "    return box_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(image_path, model, device=torch.device(\"cpu\"), topk=5):\n",
    "    ''' Predict the class (or classes) of an image using a trained deep learning model.\n",
    "    '''\n",
    "    \n",
    "    # TODO: Implement the code to predict the class from an image file\n",
    "    \n",
    "    #image preprocessing by function process_image(), it return a numpy_image\n",
    "    np_imgs = img_preprocess_for_model(image_path) \n",
    "    #transfer numpy_image to tensor_image (FloatTensor >> same datatype as model weight)\n",
    "    tensor_imgs = torch.from_numpy(np_imgs).type(torch.FloatTensor)\n",
    "    #incread to 4 dimemtion tensor using unsqueeze function \n",
    "    if device == torch.device(\"cpu\"):\n",
    "        tensor_imgs_1 = tensor_imgs.unsqueeze(0)\n",
    "    elif device == torch.device(\"cuda\"):\n",
    "        tensor_imgs_0 = tensor_imgs.unsqueeze(0).cpu()\n",
    "        tensor_imgs_1 = tensor_imgs_0.type(torch.cuda.FloatTensor)\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    #disable autograd for prediction (like validation and test)\n",
    "    with torch.set_grad_enabled(False):\n",
    "        output = model.forward(tensor_imgs_1)\n",
    "        \n",
    "    #convert output to probability by exponential func \n",
    "    probs = torch.exp(output)\n",
    "    #choosing top5 probs and idxs with topk function\n",
    "    probs_tp5, idx_tp5 = torch.topk(probs, topk)\n",
    "    \n",
    "    if device == torch.device(\"cuda\"):\n",
    "        probs_tp5_1 = probs_tp5.cpu()\n",
    "        #probs_tp5_1 = probs_tp5_cpu.type(torch.cuda.FloatTensor)\n",
    "        idx_tp5_1 = idx_tp5.cpu()\n",
    "        #idx_tp5_1 = idx_tp5_cpu.type(torch.cuda.FloatTensor)\n",
    "    else:\n",
    "        probs_tp5_1 = probs_tp5\n",
    "        idx_tp5_1 = idx_tp5\n",
    "        \n",
    "    #reload mapping dictionary from model attribute  \n",
    "    class_to_idx = model.class_to_idx\n",
    "    #reverse class_to_idx to idx_to_class\n",
    "    idx_to_class = { str(value):key for key, value in class_to_idx.items()}\n",
    "    \n",
    "    #conver top5 idx to top5 class\n",
    "    class_tp5 = [idx_to_class[str(i)] for i in idx_tp5_1[0].numpy()]\n",
    "    \n",
    "    #return top5 probability and class idx\n",
    "    \n",
    "    #reverse top5 class index to top5 name\n",
    "    top5_name = [cat_to_name[i]  for i  in class_tp5]\n",
    "    \n",
    "    return probs_tp5_1[0].numpy(), top5_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification\n",
    "def object_classification(video_path, model):\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frame_cnt = 0\n",
    "    box_frame_cnt = 0\n",
    "    boxs_list={}\n",
    "    if cap.isOpened:\n",
    "        print(\"Camera is opened\")\n",
    "    else:\n",
    "        print(\"Can't Open Camera\")\n",
    "    while(True):\n",
    "        frame_cnt+=1\n",
    "        ref, frame = cap.read()\n",
    "        if ref == False:\n",
    "            print('video_end')\n",
    "            break\n",
    "        frame0 = frame.copy()\n",
    "        try:\n",
    "            # find bounding box\n",
    "            boxs = bounding_box_pre(frame0, save_path='', show_binary=True,\n",
    "                         can1=250, can2=255, can3=3, \n",
    "                         dilate0=1, erode1=0, \n",
    "                         dilate1=0, erode2=0, \n",
    "                         dilate2=0, choose=0)\n",
    "            # filter boxs\n",
    "            boxs_f0 = box_filter(boxs)\n",
    "            boxs_f1 = remove_box_in_box(boxs_f0)\n",
    "            plot_boxs(frame0, boxs_f1)\n",
    "            # add\n",
    "            box_frame_cnt+=1\n",
    "            #boxs_list[str(box_frame_cnt)] = boxs_f1\n",
    "            \n",
    "        except:# IndexError:\n",
    "            # add\n",
    "            box_frame_cnt+=1\n",
    "            boxs_f1 = ''\n",
    "            print(\"No object\")\n",
    "        k = cv2.waitKey(1)\n",
    "        if k == ord(\" \"):\n",
    "            boxs_list[str(box_frame_cnt)] = boxs_f1\n",
    "            result_list, result_img = crop_img_with_box_list(frame, boxs_f1, model, prediction=True)\n",
    "            name = ''\n",
    "            for cls, prob in zip(result_list[0], result_list[1]):\n",
    "                name += (cls +'_' + str(round(prob, 2)) + '-')\n",
    "            name = name.rstrip('-')\n",
    "            cv2.imwrite('./'+name +'.jpg', result_img) #change img save folder\n",
    "\n",
    "        cv2.imshow(\"frame\", frame0)    \n",
    "        if k  == ord('q'):\n",
    "            break\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return boxs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0': 'Black_Tea',\n",
       " '1': 'Cheers',\n",
       " '2': 'Chrunchoco',\n",
       " '3': 'Coffee_Milk',\n",
       " '4': 'Family_Water',\n",
       " '5': 'Green_Milk_Tea',\n",
       " '6': 'LP33',\n",
       " '7': 'LS_SoyMilk',\n",
       " '8': 'Oats_Drink',\n",
       " '9': 'Oolong_Tea',\n",
       " '10': 'Oreo',\n",
       " '11': 'Puff',\n",
       " '12': 'Soy_Oats',\n",
       " '13': 'With_Kernel'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('cat_to_name_13.json', 'r') as f:\n",
    "    cat_to_name = json.load(f)\n",
    "cat_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "building resnet18 pretrain_model...\n",
      "building model classifier with full connection: \n",
      "input_size:512 \n",
      "hidden_size:256 \n",
      "output_size:14\n",
      "update model stat_dict...\n",
      "update model class_to_idx...\n",
      "rebuild model finished!!\n"
     ]
    }
   ],
   "source": [
    "model = rebuild_model(filename='resnet18_cuda_drinks_190211_ep110.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera is opened\n",
      "Do prediction now\n",
      "Do prediction now\n",
      "Do prediction now\n"
     ]
    }
   ],
   "source": [
    "check = object_classification(video_path='./Demo_T3.mp4', model=model)\n",
    "#check = object_classification(video_path=2, model=model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:my_env]",
   "language": "python",
   "name": "conda-env-my_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
